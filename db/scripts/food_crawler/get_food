#!/usr/bin/env python3
from bs4 import BeautifulSoup
import urllib.request
import re

SCHEMA = 'cs3200_project'
TABLE  = 'food'

def _debug(msg):
    print(msg, flush=True)

def _parse_page(url):
    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
    page = urllib.request.urlopen(req).read()
    soup = BeautifulSoup(page, 'html.parser')
    return soup

def _compile_to_sql(name, food_type, grams_per_serving, calories_per_100g):
    sql = f'''
INSERT INTO {SCHEMA}.{TABLE} (name, type, grams_per_serving, calories_per_100g)
VALUES ('{name}', '{food_type}', {grams_per_serving}, {calories_per_100g});\n'''
    return sql

if __name__ == '__main__':
    base_url = 'https://www.calories.info/'
    soup = _parse_page(base_url)

    # Get all links on the front page.
    regex = re.compile('calorie-link')
    a_elements = soup.find_all('a', href=True, attrs={'class': regex})
    links = []
    for link in a_elements:
        links.append(link['href'])

    for link in links:
        soup = _parse_page(link)
        regex = re.compile('^kt-row')
        rows = soup.find_all('tr', attrs={'class': regex})
        link_arr = link.split('/')
        food_type = link_arr[len(link_arr) - 1]
        with open(f'./data/{food_type}.sql', '+w') as f:
            _debug(f'Writing {food_type} to file')
            for row in rows:
                cells = row.find_all('td')
                name = cells[0].a.getText()
                
                serving = cells[2].getText()
                g_str = re.compile('[0-9]+ g|[0-9]+ ml')
                num_val = re.compile('[0-9]+')
                grams_per_serving = re.search(num_val, re.search(g_str, serving).group()).group()

                cal_str = cells[4].getText()
                calories_per_100g = re.search(num_val, cal_str).group()

                sql = _compile_to_sql(name, food_type, grams_per_serving, calories_per_100g)

                f.write(sql)